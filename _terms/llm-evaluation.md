---
id: llm-evaluation
title: 大模型评测
title_en: LLM Evaluation
category: safety
type: concept
order: 85
aliases: [LLM Evaluation, 评测, llm-evaluation-placeholder]
keywords: [LLM Evaluation, 评测, Benchmark, MMLU, MT-Bench, 红队, Evals]
brief: 评估大语言模型在能力、可靠性与安全性上的表现的方法体系，涵盖基准测试、人工评审、在线指标与红队测试等。
meta: [基准测试, 安全评估, 线上监控]
---

大模型评测（LLM Evaluation）指一整套用于衡量模型“能做什么、做得多好、在什么边界会出问题”的方法与流程。它既包括离线基准（benchmark），也包括面向真实产品的在线观测与安全测试。

### 评测对象通常包含什么
- **能力**：知识、推理、代码、工具使用、多语言、多模态等。
- **可靠性**：一致性、可复现性、遵循格式、对抗性输入下的稳定性。
- **安全与合规**：越狱、隐私泄露、偏见、有害内容、越权工具调用等风险。
- **体验**：可用性、帮助度、语气风格、拒答质量、引用与可追溯性。

### 常见方法（从离线到在线）
1. **离线基准测试**：如通识/学科题库、对话评测、代码题集等，适合做版本对比与回归。
2. **人工评审（Human Eval）**：对“有主观性/开放式”的任务更可靠，但成本较高，需设计一致的评审标准。
3. **LLM-as-a-Judge**：用模型当裁判做打分/排序，成本低但可能有偏置与“同源偏好”，需要校准与抽检。
4. **红队与安全审计**：覆盖越狱、提示注入、数据外泄、越权调用等高风险场景，强调覆盖面与可追责。
5. **在线指标与回放**：在真实流量下监控拒答率、用户改写率、人工接管率、延迟与成本，并用日志回放做回归测试。

### 为什么“只看一个分数”不够
- 基准通常代表“某类题型”，不等于真实业务；分数提升也可能来自数据泄漏或提示工程差异。
- 不同维度之间会有权衡：更强的拒答可能牺牲帮助度；更激进的工具调用可能提高效率但增大风险。

### 实践建议（可落地的最小集合）
- **为你的业务定义一套“小而精”的回归集**：覆盖高频任务、关键边界与已知事故案例。
- **把评测当成工程管线**：每次改 prompt、工具、检索、模型版本，都能自动跑回归并产出差异报告。
- **分层看指标**：能力（离线）+ 可靠性（格式/一致性）+ 安全（红队）+ 体验（在线）。

### 相关词条
- 基准：{{ '/terms/mmlu/' | relative_url }}、{{ '/terms/mt-bench/' | relative_url }}
- 安全：{{ '/terms/red-teaming/' | relative_url }}、{{ '/terms/ai-safety/' | relative_url }}

