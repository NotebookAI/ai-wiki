---
id: alignment
title: 对齐（Alignment）
title_en: Alignment
category: safety
type: concept
order: 75
aliases: [对齐, Alignment]
keywords: [Alignment, 对齐, 人类偏好, 价值观]
brief: 让模型的行为、价值观和偏好尽可能与人类目标和社会规范保持一致的技术与方法集合。
meta: [人类偏好, 价值对齐]
---

在大模型语境下，对齐通常包含两个层次：一是让模型理解并遵守明确的指令与约束（如不输出违法、有害内容），二是让模型在模糊情境下尽量体现人类价值观与社会责任。SFT、RLHF、DPO、RLAIF 等对齐方法正是为此服务。

对齐并非一次性工作，而是伴随模型迭代与应用扩展持续进行，需要技术手段与组织流程共同配合。

